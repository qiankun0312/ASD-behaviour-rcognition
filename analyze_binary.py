import pickle
import numpy as np
import torch
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams["font.family"] = ["SimHei"]
plt.rcParams['axes.unicode_minus'] = False  # æ­£ç¡®æ˜¾ç¤ºè´Ÿå·

def load_results(pkl_path):
    """åŠ è½½pklæ–‡ä»¶ä¸­çš„ç»“æœ"""
    with open(pkl_path, 'rb') as f:
        results = pickle.load(f)
    print(f"æˆåŠŸåŠ è½½ {len(results)} ä¸ªæ ·æœ¬")
    return results

def extract_labels_and_scores(results):
    """ä»ç»“æœä¸­æå–çœŸå®æ ‡ç­¾ã€é¢„æµ‹æ ‡ç­¾å’Œé¢„æµ‹åˆ†æ•°"""
    gt_labels = []
    pred_labels = []
    pred_scores = []  # æ­£ç±»çš„æ¦‚ç‡åˆ†æ•°
    
    for result in results:
        # æå–çœŸå®æ ‡ç­¾
        if isinstance(result['gt_label'], torch.Tensor):
            gt_label = result['gt_label'].item()
        else:
            gt_label = result['gt_label']
        gt_labels.append(gt_label)
        
        # æå–é¢„æµ‹æ ‡ç­¾
        if isinstance(result['pred_label'], torch.Tensor):
            pred_label = result['pred_label'].item()
        else:
            pred_label = result['pred_label']
        pred_labels.append(pred_label)
        
        # æå–é¢„æµ‹åˆ†æ•°ï¼ˆå–ç¬¬äºŒä¸ªç±»åˆ«çš„åˆ†æ•°ä½œä¸ºæ­£ç±»æ¦‚ç‡ï¼‰
        if isinstance(result['pred_score'], torch.Tensor):
            pred_score = result['pred_score'].numpy()
        else:
            pred_score = np.array(result['pred_score'])
        
        # å‡è®¾ç±»åˆ«1æ˜¯æ­£ç±»ï¼ˆASDè¡Œä¸ºï¼‰ï¼Œå–ç¬¬äºŒä¸ªåˆ†æ•°
        if len(pred_score) == 2:
            pred_scores.append(pred_score[1])  # æ­£ç±»æ¦‚ç‡
        else:
            # å¦‚æœåªæœ‰ä¸€ä¸ªåˆ†æ•°ï¼Œå‡è®¾æ˜¯äºŒåˆ†ç±»çš„logits
            pred_scores.append(pred_score[0] if len(pred_score) == 1 else pred_score[1])
    
    return np.array(gt_labels), np.array(pred_labels), np.array(pred_scores)

def calculate_metrics(gt_labels, pred_labels, pred_scores, positive_class=1, class_names=None):
    """è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡"""
    
    if class_names is None:
        class_names = ['Class_0', 'Class_1']
    
    # åŸºç¡€åˆ†ç±»æŒ‡æ ‡
    accuracy = accuracy_score(gt_labels, pred_labels)
    precision = precision_score(gt_labels, pred_labels, average='binary', pos_label=positive_class)
    recall = recall_score(gt_labels, pred_labels, average='binary', pos_label=positive_class)
    f1 = f1_score(gt_labels, pred_labels, average='binary', pos_label=positive_class)
    
    # å¤šç±»åˆ«å¹³å‡æŒ‡æ ‡ï¼ˆå¯¹äºäºŒåˆ†ç±»ï¼Œmacroå’Œweightedé€šå¸¸ç›¸åŒï¼‰
    precision_macro = precision_score(gt_labels, pred_labels, average='macro')
    recall_macro = recall_score(gt_labels, pred_labels, average='macro')
    f1_macro = f1_score(gt_labels, pred_labels, average='macro')
    
    precision_weighted = precision_score(gt_labels, pred_labels, average='weighted')
    recall_weighted = recall_score(gt_labels, pred_labels, average='weighted')
    f1_weighted = f1_score(gt_labels, pred_labels, average='weighted')
    
    # ROC-AUC
    try:
        roc_auc = roc_auc_score(gt_labels, pred_scores)
    except Exception as e:
        print(f"è®¡ç®—ROC-AUCæ—¶å‡ºé”™: {e}")
        roc_auc = None
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(gt_labels, pred_labels)
    
    # å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡
    class_report = classification_report(gt_labels, pred_labels, target_names=class_names, output_dict=True)
    
    metrics = {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'precision_macro': precision_macro,
        'recall_macro': recall_macro,
        'f1_macro': f1_macro,
        'precision_weighted': precision_weighted,
        'recall_weighted': recall_weighted,
        'f1_weighted': f1_weighted,
        'roc_auc': roc_auc,
        'confusion_matrix': cm,
        'classification_report': class_report,
        'gt_labels': gt_labels,
        'pred_labels': pred_labels
    }
    
    return metrics

def plot_confusion_matrix(cm, class_names, save_path):
    """ç»˜åˆ¶å¹¶ä¿å­˜æ··æ·†çŸ©é˜µ"""
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names,
                cbar_kws={'shrink': 0.8})
    plt.title('Confusion Matrix', fontsize=16, fontweight='bold')
    plt.ylabel('True Label', fontsize=12)
    plt.xlabel('Predicted Label', fontsize=12)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³: {save_path}")

def plot_roc_curve(gt_labels, pred_scores, save_path):
    """ç»˜åˆ¶ROCæ›²çº¿"""
    from sklearn.metrics import roc_curve
    
    fpr, tpr, thresholds = roc_curve(gt_labels, pred_scores)
    roc_auc = roc_auc_score(gt_labels, pred_scores)
    
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random classifier')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate', fontsize=12)
    plt.ylabel('True Positive Rate', fontsize=12)
    plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=16, fontweight='bold')
    plt.legend(loc="lower right")
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"ROCæ›²çº¿å·²ä¿å­˜è‡³: {save_path}")

def print_detailed_metrics(metrics, class_names=['Typical', 'ASD']):
    """æ‰“å°è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡"""
    
    print("\n" + "="*70)
    print("STGCN æ¨¡å‹æ€§èƒ½è¯„ä¼°ç»“æœ")
    print("="*70)
    
    print(f"\nğŸ“Š åŸºç¡€æŒ‡æ ‡:")
    print("-" * 40)
    print(f"å‡†ç¡®ç‡ (Accuracy):          {metrics['accuracy']:.4f}")
    print(f"ç²¾ç¡®ç‡ (Precision):         {metrics['precision']:.4f}")
    print(f"å¬å›ç‡ (Recall):            {metrics['recall']:.4f}")
    print(f"F1åˆ†æ•° (F1-Score):          {metrics['f1_score']:.4f}")
    
    if metrics['roc_auc'] is not None:
        print(f"ROC-AUC:                    {metrics['roc_auc']:.4f}")
    
    print(f"\nğŸ“ˆ å®å¹³å‡æŒ‡æ ‡:")
    print("-" * 40)
    print(f"å®å¹³å‡ç²¾ç¡®ç‡:               {metrics['precision_macro']:.4f}")
    print(f"å®å¹³å‡å¬å›ç‡:               {metrics['recall_macro']:.4f}")
    print(f"å®å¹³å‡F1åˆ†æ•°:               {metrics['f1_macro']:.4f}")
    
    print(f"\nâš–ï¸  åŠ æƒå¹³å‡æŒ‡æ ‡:")
    print("-" * 40)
    print(f"åŠ æƒå¹³å‡ç²¾ç¡®ç‡:             {metrics['precision_weighted']:.4f}")
    print(f"åŠ æƒå¹³å‡å¬å›ç‡:             {metrics['recall_weighted']:.4f}")
    print(f"åŠ æƒå¹³å‡F1åˆ†æ•°:             {metrics['f1_weighted']:.4f}")
    
    print(f"\nğŸ¯ å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡:")
    print("-" * 40)
    for class_name in class_names:
        if class_name in metrics['classification_report']:
            report = metrics['classification_report'][class_name]
            print(f"{class_name}:")
            print(f"  ç²¾ç¡®ç‡: {report['precision']:.4f}")
            print(f"  å¬å›ç‡: {report['recall']:.4f}")
            print(f"  F1åˆ†æ•°: {report['f1-score']:.4f}")
            print(f"  æ”¯æŒæ•°: {report['support']}")
    
    print(f"\nğŸ“‹ æ€»ä½“ç»Ÿè®¡:")
    print("-" * 40)
    print(f"æ€»æ ·æœ¬æ•°: {len(metrics['gt_labels'])}")
    print(f"å‡†ç¡®ç‡:   {metrics['accuracy']:.4f}")
    
    print(f"\nğŸ”¢ æ··æ·†çŸ©é˜µ:")
    print("-" * 40)
    print(metrics['confusion_matrix'])

def save_metrics_to_file(metrics, save_path, class_names=['Typical_Behavior', 'ASD_Behavior']):
    """å°†æŒ‡æ ‡ä¿å­˜åˆ°æ–‡ä»¶"""
    with open(save_path, 'w',encoding='utf-8') as f:
        f.write("STGCNæ¨¡å‹è¯„ä¼°ç»“æœ\n")
        f.write("="*50 + "\n\n")
        
        f.write("åŸºç¡€æŒ‡æ ‡:\n")
        f.write(f"å‡†ç¡®ç‡ (Accuracy): {metrics['accuracy']:.4f}\n")
        f.write(f"ç²¾ç¡®ç‡ (Precision): {metrics['precision']:.4f}\n")
        f.write(f"å¬å›ç‡ (Recall): {metrics['recall']:.4f}\n")
        f.write(f"F1åˆ†æ•° (F1-Score): {metrics['f1_score']:.4f}\n")
        if metrics['roc_auc'] is not None:
            f.write(f"ROC-AUC: {metrics['roc_auc']:.4f}\n")
        
        f.write("\næ··æ·†çŸ©é˜µ:\n")
        f.write(str(metrics['confusion_matrix']))
        f.write("\n\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\n")
        
        # ä½¿ç”¨ classification_report ç”Ÿæˆå­—ç¬¦ä¸²æŠ¥å‘Š
        report_str = classification_report(
            metrics['gt_labels'], 
            metrics['pred_labels'], 
            target_names=class_names
        )
        f.write(report_str)
    
    print(f"è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {save_path}")

def main(pkl_path, output_dir='evaluation_results', positive_class=1):
    """ä¸»å‡½æ•°"""
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(output_dir).mkdir(exist_ok=True)
    
    # ç±»åˆ«åç§°ï¼ˆæ ¹æ®æ‚¨çš„æ•°æ®é›†è°ƒæ•´ï¼‰
    class_names = ['Typical_Behavior', 'ASD_Behavior']
    
    # åŠ è½½ç»“æœ
    print("æ­£åœ¨åŠ è½½ç»“æœæ–‡ä»¶...")
    results = load_results(pkl_path)
    
    # æå–æ ‡ç­¾å’Œåˆ†æ•°
    print("æ­£åœ¨æå–æ ‡ç­¾å’Œé¢„æµ‹åˆ†æ•°...")
    gt_labels, pred_labels, pred_scores = extract_labels_and_scores(results)
    
    # è®¡ç®—æŒ‡æ ‡
    print("æ­£åœ¨è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    metrics = calculate_metrics(gt_labels, pred_labels, pred_scores, positive_class, class_names)
    
    # æ‰“å°ç»“æœ
    print_detailed_metrics(metrics, class_names)
    
    # ç»˜åˆ¶å›¾è¡¨
    plot_confusion_matrix(metrics['confusion_matrix'], class_names, 
                         f'{output_dir}/confusion_matrix.png')
    
    if metrics['roc_auc'] is not None:
        plot_roc_curve(gt_labels, pred_scores, f'{output_dir}/roc_curve.png')
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    save_metrics_to_file(metrics, f'{output_dir}/evaluation_results.txt', class_names)
    
    # ä¿å­˜ç»“æ„åŒ–æ•°æ®
    results_data = {
        'gt_labels': gt_labels,
        'pred_labels': pred_labels,
        'pred_scores': pred_scores,
        'metrics': metrics
    }
    with open(f'{output_dir}/evaluation_data.pkl', 'wb') as f:
        pickle.dump(results_data, f)
    
    print(f"\nâœ… æ‰€æœ‰è¯„ä¼°å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {output_dir}/")
    
    return metrics

if __name__ == "__main__":
    # ä½¿ç”¨ç¤ºä¾‹
    pkl_file_path = "test/result_2_pp.pkl"  # æ›¿æ¢ä¸ºæ‚¨çš„pklæ–‡ä»¶è·¯å¾„
    output_directory = "evaluation_results"
    
    # è¿è¡Œè¯„ä¼°
    metrics = main(pkl_file_path, output_directory, positive_class=1)